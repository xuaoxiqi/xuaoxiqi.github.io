---
layout: main
description: Research on OpenMP.
title: Research_OpenMP
---

    <div id="divTitleNav">
        <a href="./research.html"><img src="./images/left-arrow.png" class="navDock" /></a>
    </div>
    
<div id="divPost">

    <article id="articlePost">
    <br/>
 
    <h2>Open Multi-Processing (OpenMP)</h2>
            <hr/>

    <p>
Open Multi-Processing (OpenMP) is an application programming interface (API) that supports shared memory multiprocessing programming.
It fits shared-memory parallel computers (SMPs).
    </p>
    
    <h3>Sample OpenMP code</h3>
    
    <h4>Compile</h4>
<p>The simplest ways to compile the codes are: </p>
			<pre> <code>
$ <font color="red">ifort -qopenmp</font> calPi_OpenMP.F90
$ <font color="red">gfortran -fopenmp</font> calPi_OpenMP.F90
			</code> </pre>
Running: $ ./a.out

<p>Belows are examples of adding compiler flag to various compilers in OpenMP code:</p>
			<pre> <code>
$ <font color="red">ifort -qopenmp</font> <font color="blue">-O2 -fp-model precise</font> calPi_OpenMP.F90 <font color="green">-o calPi</font>
$ <font color="red">gfortran -fopenmp</font> <font color="blue">-O2 -fcheck=all</font> calPi_OpenMP.F90 <font color="green">-o calPi</font>
$ <font color="red">ifort -qopenmp</font> <font color="blue">-check all</font> calPi_OpenMP.F90 <font color="green">-o calPi</font>

<del>$ <font color="red">pgf90 -mp</font> calPi_OpenMP.F90 -o calPi</del> (obsoleted)
			</code> </pre>
            
On Tianhe-2, job.sh file:
			<pre> <code>
#!/bin/bash
module load intelcompiler/18.0.0
yhrun -N 1 -n 1 -c 24 -p bigdata ./calPi
			</code> </pre>
Running: $ yhbatch -N 1 -p bigdata ./job.sh
            
<p>To set the number of running treads, one may set the enviroment variables as</p>
			<pre> <code>
export OMP_NUM_THREADS=4 && ./calPi
			</code> </pre>

or use conditional compilation as
			<pre> <code>
#ifdef _OPENMP
    call OMP_set_num_threads(2)
    myMaxThreads = OMP_get_max_threads()
    write(*,*) "Max Running threads=",myMaxThreads
#endif
			</code> </pre>
            
<p>The preprocessing is activated with a compile-time flag, i.e., <b>-cpp</b> for gfortran, <b>-fpp</b> for ifort. 
Another trick is simply change the file name from <b>*.f90</b> to <b>*.F90</b>.</p>
    
    
    <h4>Example I: Calculate Pi</h4>
    <embed src="./codes/calPi_OpenMP.F90" width="100%" height="1200">
    
    <h4>Example II: Poisson Solver</h4>
    <embed src="./codes/Jacobi_OpenMP.F90" width="100%" height="4700">
    
    <h3>OpenMP in lattice Boltzmann method</h3>
    <P>
Standard lattice Boltzmann method (LBM) mainly contains three parts: collision, streaming, and calculating macroscopic variables.
The first and the last parts are quite easy to parallelize, due to the space locality of the numerical algorithm.
As a matter of fact, a  <i>PARALLEL DO</i> construct is enough. 
In streaming step, the distribution function exchange the information with its neighboring node.
    </p>
            
    <h4>Example: Lid driven cavity flow</h4>
    <p>
In 2D test, the mesh size 1024*1024, and the iterative time is 10000;
In 3D test, the mesh size 128*128*128, and the iterative time is 10000.
Each case is repeated 3 times to reduce the random error.
    </p>
    <p>
The operating system is Linux, and the compiler is ifort.
    </p>
    <img src="./images/research/HPC/OpenMP/OpenMP-2D.png" width="49%"></img>
    <img src="./images/research/HPC/OpenMP/OpenMP-3D.png" width="49%"></img>
    
    <h4>Speedup and Efficiency</h4>
    
    <img src="./images/research/HPC/OpenMP/speedupEfficiency.png" width="49%"></img>
    

    <p>
The code scales to 16 threads while maintaining parallel efficiency above 50%.
However, further increasing the number of threads lead to subtitle performance optimization (or may even degrade the speedup due to the overhead).
    </p>
    
    <h3>High performance computing techniques (Read more...)</h3>
        <ul class="ulResearch">
            <li class="liResearch"><a href="./research_HPC_OpenMP.html">Open Multi-Processing (OpenMP)</a></li>
            <li class="liResearch"><a href="./research_HPC_MPI.html">Message Passing Interface (MPI)</a></li>
            <li class="liResearch"><a href="./research_HPC_OpenACC.html">Open Accelerators (OpenACC)</a></li>
        </ul>
        
    </article>
    
    <div class="div0"></div>
    
</div>
