---
layout: main
description: Research on MPI.
title: Research_MPI
---
    <div id="divTitleNav">
        <a href="./research.html"><img src="./images/left-arrow.png" class="navDock" /></a>
    </div>
    
<div id="divPost">

    <article id="articlePost">
    <br/>
 
    <h2>Message Passing Interface (MPI)</h2>
            <hr/>

    <p>
Message Passing Interface (MPI) supports distributed memory system consisting of multiple computers with independent memory.
    </p>
    
    <h3>Sample MPI code</h3>
       
    <h4>Compile (only MPI)</h4>
			<pre> <code>
$ <font color="red">mpif90</font> -O2 calPi_MPI.F90 <font color="green">-o test</font>
$ <font color="red">mpiifort</font> -O2 calPi_MPI.F90 <font color="green">-o test</font>
$ nohup mpirun -np 8 ./test &
			</code> </pre>
    
    <h4>Compile (hybrid MPI and OpenMP)</h4>
<p>Case1: On local machine:</p>
			<pre> <code>
$ <font color="red">mpif90</font> -fopenmp -O2 test_MPI.F90 -o test
$ <font color="red">mpiifort</font> -qopenmp -O2 test_MPI.F90 -o test
Running:
export OMP_NUM_THREADS=12 && nohup mpirun -np 2 ./test &
			</code> </pre>
            
<p>Case 2: On Tianhe-2:</p>
			<pre> <code>
$ <font color="red">mpif90</font> -openmp <font color="blue">-heap-arrays</font> -O2 test_MPI.F90 -o test
job.sh file:
#!/bin/bash
export OMP_NUM_THREADS=12 && yhrun -N 2 -n 4 -c 12 -p bigdata ./test
Running:
yhbatch -N 2 -p bigdata ./job.sh
			</code> </pre>
            
    <h4>PBS file</h4>
			<pre> <code>
# !/bin/bash
#PBS -N mpi2d
#PBS -l nodes=2:ppn=2
#PBS -j oe output.out
#PBS -l walltime=1000000:00:00
cd /DATA2/work/AoXu/testMPI_2D/
export OMP_NUM_THREADS=12
mpirun -hostfile $PBS_NODEFILE /DATA2/work/AoXu/testMPI_2D/mpi2d
			</code> </pre>
            
    <embed src="./images/research/HPC/MPI_bindings.htm" width="100%" height="800">
    
    <h4>Example I: Calculate Pi</h4>
    <embed src="./codes/calPi_MPI_OpenMP.F90" width="100%" height="2200">
    <img src="./images/research/HPC/MPI_startEnd.png" width="35%"></img>
    
    <h4>Example II: Poisson Solver</h4>
    <embed src="./codes/Jacobi_MPI.F90" width="100%" height="2000">
    <img src="./images/research/HPC/MPI_1D_Partitioning.png" width="27%"></img>
    <img src="./images/research/HPC/MPI_1D_Partitioning-2.png" width="27%"></img>
    
    
    <embed src="./codes/Jacobi_MPI_block.F90" width="100%" height="2000">
    <img src="./images/research/HPC/MPI_2D_Partitioning.png" width="27%"></img>
    <img src="./images/research/HPC/MPI_2D_Partitioning-2.png" width="27%"></img>
    
    <h3>High performance computing techniques (Read more...)</h3>
        <ul class="ulResearch">
            <li class="liResearch"><a href="./research_HPC_OpenMP.html">Open Multi-Processing (OpenMP)</a></li>
            <li class="liResearch"><a href="./research_HPC_MPI.html">Message Passing Interface (MPI)</a></li>
            <li class="liResearch"><a href="./research_HPC_OpenACC.html">Open Accelerators (OpenACC)</a></li>
        </ul>
        
    </article>
    
    <div class="div0"></div>
    
</div>
